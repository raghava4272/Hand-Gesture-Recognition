{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def recognize_gesture(hand_landmarks):\n",
    "    # Example gesture recognition logic\n",
    "    if not hand_landmarks:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # Assuming gestures based on the position of landmarks\n",
    "    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
    "\n",
    "    # Example gesture recognition logic:\n",
    "    # Recognize 'hi' if the thumb is extended and fingers are closed\n",
    "    thumb_tip = landmarks[mp_hands.HandLandmark.THUMB_TIP.value]\n",
    "    index_tip = landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP.value]\n",
    "    middle_tip = landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP.value]\n",
    "    ring_tip = landmarks[mp_hands.HandLandmark.RING_FINGER_TIP.value]\n",
    "    pinky_tip = landmarks[mp_hands.HandLandmark.PINKY_TIP.value]\n",
    "\n",
    "    # Simple logic to determine \"hi\"\n",
    "    if (index_tip[1] < thumb_tip[1] and\n",
    "        middle_tip[1] < thumb_tip[1] and\n",
    "        ring_tip[1] < thumb_tip[1] and\n",
    "        pinky_tip[1] < thumb_tip[1]):\n",
    "        return \"hi\"\n",
    "\n",
    "    # More complex logic needed for \"fuck you\", for now a placeholder\n",
    "    if (index_tip[1] < thumb_tip[1] and\n",
    "        middle_tip[1] > thumb_tip[1] and\n",
    "        ring_tip[1] > thumb_tip[1] and\n",
    "        pinky_tip[1] > thumb_tip[1]):\n",
    "        return \"fuck you\"\n",
    "\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727885098.070072 10578168 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1727885098.079104 10578670 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1727885098.089466 10578670 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2024-10-02 21:34:58.449 Python[51460:10578168] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp_hands\u001b[38;5;241m.\u001b[39mHands(min_detection_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, min_tracking_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hands:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m---> 61\u001b[0m         ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize MediaPipe Hands and Drawing Utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def recognize_gesture(hand_landmarks):\n",
    "    if not hand_landmarks:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
    "\n",
    "    thumb_tip = landmarks[mp_hands.HandLandmark.THUMB_TIP.value]\n",
    "    index_tip = landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP.value]\n",
    "    middle_tip = landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP.value]\n",
    "    ring_tip = landmarks[mp_hands.HandLandmark.RING_FINGER_TIP.value]\n",
    "    pinky_tip = landmarks[mp_hands.HandLandmark.PINKY_TIP.value]\n",
    "\n",
    "    # Recognize \"hi\"\n",
    "    if (index_tip[1] < thumb_tip[1] and\n",
    "        middle_tip[1] < thumb_tip[1] and\n",
    "        ring_tip[1] < thumb_tip[1] and\n",
    "        pinky_tip[1] < thumb_tip[1]):\n",
    "        return \"hi\"\n",
    "\n",
    "    # Recognize \"ONE\"\n",
    "    if (index_tip[1] < thumb_tip[1] and\n",
    "        middle_tip[1] > thumb_tip[1] and\n",
    "        ring_tip[1] > thumb_tip[1] and\n",
    "        pinky_tip[1] > thumb_tip[1]):\n",
    "        return \"double fuck u\"\n",
    "\n",
    "    # Recognize \"Thumbs Up\"\n",
    "    if (index_tip[1] < thumb_tip[1] and\n",
    "        middle_tip[1] < thumb_tip[1] and\n",
    "        ring_tip[1] < thumb_tip[1] and\n",
    "        pinky_tip[1] < thumb_tip[1] and\n",
    "        thumb_tip[1] < index_tip[1]):\n",
    "        return \"Thumbs Up\"\n",
    "\n",
    "    # Recognize \"Peace\"\n",
    "    if (index_tip[1] < thumb_tip[1] and\n",
    "        middle_tip[1] < thumb_tip[1] and\n",
    "        ring_tip[1] > thumb_tip[1] and\n",
    "        pinky_tip[1] > thumb_tip[1] and\n",
    "        (middle_tip[1] - index_tip[1]) < 0.05):\n",
    "        return \"Peace\"\n",
    "\n",
    "    # Recognize \"Rock On\"\n",
    "    if (index_tip[1] < thumb_tip[1] and\n",
    "        middle_tip[1] > thumb_tip[1] and\n",
    "        ring_tip[1] > thumb_tip[1] and\n",
    "        pinky_tip[1] < thumb_tip[1]):\n",
    "        return \"Rock On\"\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.flip(image, 1)\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        gesture_text = \"Unknown\"\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                gesture_text = recognize_gesture(hand_landmarks)\n",
    "\n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                          mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                          mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Overlay the recognized gesture text on the frame\n",
    "        cv2.putText(image, f'Gesture: {gesture_text}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Hand Tracking', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
